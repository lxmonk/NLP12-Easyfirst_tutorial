Tutorial: An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing -- writing the code
======================================================================================================

Author: Aviad Reich
Date: 2012-07-21 Sat



Table of Contents
=================
1 How to contribute to this document
    1.1 All formats
    1.2 OPTION: Using Emacs' org-mode
2 An intro to dep parsing [0/3]
    2.1 Formats of the data
    2.2 Example sentences
    2.3 Rendering of trees in text and graphically
3 Intro to dep parsing using transition based parsing [0/2]
    3.1 How shift reduce works
    3.2 Turning a tree into a sequence of shift reduce transitions
4 Malt like parsing [0/2]
    4.1 Training a classifier to learn which transition is best at each step
    4.2 Typical features used for malt
5 Evaluation methods for dep parsing
6 Evaluation of a demo malt parser
7 Easy First
    7.1 Read the paper
    7.2 Quiz on the paper
        7.2.1 Introduction and Easy-First
        7.2.2 Parsing Algorithm
        7.2.3 Learning Algorithm and Feature Representation
        7.2.4 Computational Complexity and Efficient Implementation
        7.2.5 Perceptron Classifier
        7.2.6 Cython
8 Cython Primer
9 Code
    9.1 easyfirst.py:
    9.2 ml.pyx:


1 How to contribute to this document 
-------------------------------------
  

1.1 All formats 
================
   This document exists in XHTML, latex, text and org-mode
   versions. Editing the first three is straightforward, but will force
   all future contributors (if such will exist) to use the same
   version/format, since this will break the compatibility of the
   different formats. Do this only if you're sure the future contributors
   will be OK with it, or you know they will not exist (for example: you
   are now completing the document). If you have done so, be sure to
   either label the other versions as outdated/irrelevant, or simply
   delete them.

1.2 OPTION: Using Emacs' org-mode 
==================================
   Another option is to edit the text file 'tutorial.org' in you favorite
   text editor. If you ignore lines 5-15, it's pretty much
   straightforward to understand the usage: 
   The number of left-aligned '*' before a line determine it's `depth' in
   the document (more is deeper), and elements are automatically nested in
   the immediate higher level item. 

   if you want $\TeX{}$, or $\LaTeX$,
   include its code inside adjacent dollar signs: 
   =$[latex code here]$=.
   Just look at 'tutorial.org' for examples.

   To use the other many wonderful, yet simple to use, features of
   org-mode, like auto-numbering of items, footnotes and others, 
   it's recommended you read the [orgmode website] and the (relevant) [docs],
   or the tutorial that comes with it as part of Emacs, or [these tutorials].

   TODOs: When (if..) you complete an item that is marked as a red
             'TODO', just delete the word 'TODO' from the =org=
             file. Once a whole section is done, delete the topmost
             'TODO', and the '[x/x]' after the section name, so they will
             no longer be displayed. You could optionally move the cursor
             to the line with a 'TODO' and press Ctrl-c Ctrl-t to mark
             that item as done (and add a timestamp).  An additional
             Ctrl-c Ctrl-t will remove the TODO altogether.


    Exporting: To export to HTML, tex and text once you're done, open
             Emacs (you  know where to get it), press Ctrl-x Ctrl-f
             and type the path to the file. Then, press Ctrl-c
             Ctrl-e h (for HTML), Ctrl-c Ctrl-e p (for pdf), Ctrl-c
             Ctrl-e l (for latex), Ctrl-c Ctrl-e u (for unicode
             text, a for ascii). Many other export format exist -
             you'll find it in the ``Emacs-style popup''
             window. 
             *MAKE SURE YOU EXPORT IN ALL FORMATS ONCE YOU'RE
             DONE, SO COMPATIBILITY IS MAINTAINED.* 

As this is written (August 2012), I care about this document, and
would be happy to extend my help if it's wanted. To email me use the
first 3 letters of `Aviad', followed by a dot ('.') and the
first 3 of `Reich'. Then mail me at: =[what-you-got]@gmail.com=.

Additionally, there is a github repo:
[https://github.com/lxmonk/NLP12-Easyfirst_tutorial], that you can clone
or fork. If you do, and you've created a new one - change this
address. Otherwise, let me know and I'll update.



[orgmode website]: http://orgmode.org/
[docs]: http://orgmode.org/org-mode-documentation.html
[these tutorials]: http://orgmode.org/worg/org-tutorials/

2 TODO An intro to dep parsing [0/3] 
-------------------------------------


2.1 TODO Formats of the data 
=============================

2.2 TODO Example sentences 
===========================

2.3 TODO Rendering of trees in text and graphically 
====================================================

3 TODO Intro to dep parsing using transition based parsing [0/2] 
-----------------------------------------------------------------

3.1 TODO How shift reduce works 
================================

3.2 TODO Turning a tree into a sequence of shift reduce transitions 
====================================================================

4 TODO Malt like parsing [0/2] 
-------------------------------

4.1 TODO Training a classifier to learn which transition is best at each step 
==============================================================================

4.2 TODO Typical features used for malt 
========================================

5 TODO Evaluation methods for dep parsing 
------------------------------------------

6 TODO Evaluation of a demo malt parser 
----------------------------------------

7 Easy First 
-------------
  

7.1 Read the paper 
===================

The article: 
*Easy First Dependency Parsing of Modern Hebrew*, 
   Yoav Goldberg and Michael Elhadad, 
   /SPMRL 2010 (NAACL Workshop on Statistical Parsing of
   Morphologically-rich Languages)/


It can be obtained from [Yoav Goldberg's BGU webpage], or at the acm
website: 
[http://dl.acm.org/citation.cfm?id=1857999.1858114].


   

[Yoav Goldberg's BGU webpage]: http://www.cs.bgu.ac.il/~yoavg/publications/naacl2010dep.pdf

7.2 Quiz on the paper 
======================

7.2.1 Introduction and Easy-First 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1) From the article (pg. 1): 
     "Current dependency parsers can be categorized
     into three families: *local-and-greedy transition-based parsers*
     (e.g., MALTPARSER (Nivre et al., 
     2006)), *globally optimized graph-based parsers*
     (e.g., MST P ARSER (McDonald et al., 2005)), and
     *hybrid systems* (e.g., (Sagae and Lavie, 2006b;
     Nivre and McDonald, 2008)), which combine the
     output of various parsers into a new and improved
     parse, and *which are orthogonal to our approach*."  (no emphasis   
   in the original text) 
   _Who is orthogonal to the authors' approach? Why?_ 
   a: Local-and-greedy transition-based parsers
   b: Globally optimized graph-based parsers
   c: Hybrid systems
   d: All of the above
   e: None of the above
   
2) According to the article, what are the shortcomings of
   transition-based parsers? Which of these have been addressed by the
   easy-first parser? How?

3) One might claim that transition-based parsers suffer from an
   imbalance, in relation to the knowledge they have about the
   sentence as it's being parsed. How might easy-first help to mend
   this? 
   
4) Why are transition-based parsers often restricted to only a limited
   look-ahead window? 
   
5) When will we prefer a globally optimized graph-based parser over a
   transition-based parser? When would a transition-based one be more
   appropriate? 
   
6) The article states that easy-first is a greedy algorithm. What are
   the benefits of this fact? 

7) How are transition-based parsers different than humans when
   dependency-parsing a sentence? 

8) Do humans annotate a sentence in a way similar to graph-based
   parsers? 

9) Of the three parser classes (transition, graph-based and
   easy-first), which is the most similar to a human's way of
   annotating a sentence? 

10) In your opinion, is being similar to the human way of parsing a
    sentence a positive or a negative approach to the problem? Why?

7.2.2 Parsing Algorithm 
~~~~~~~~~~~~~~~~~~~~~~~~

1) Look at figure 1 in the article. In each step, how is the action
    to be performed chosen?

2) In step 1 (figure 1), if the bold number *403* was instead 136,
   what action would have been performed? 

3) What is the range of values for the arc "brown --> fox", assuming
   all others remain unchanged, that will lead to the same parse tree?
   What is the range for the arcs "a --> brown" and "joy --> with"
   that will lead to the same parse tree?

4) Assuming the *difficulty* of choosing an action is measured by the
   difference between the two highest arc's score. On which step was
   making this decision hardest? Can you "feel" this difficulty trying
   to parse the sentence yourself?

5) Algorithm 1.1 is identical to algorithm 1 (see below), but in it, line 3 is *changed* to
   $pending = p_{1} \ldots p_{n-1} ‚Üê w_{1} \ldots w_{n-1}$.
   What will be the first step in parsing the sentence "a brown fox
   jumped with joy" in which the two algorithms will diverge? (hint:
   use figure 1) 
   In general, what will this change cause? 
   [file:images/Algorithm1.png]]] 
   
6) What does the function EdgeFor do? How?
   
7) Write the loop from line 5 in python.

8) Can you find this loop in the file [easyfirst.py]? 
   hint 1: the variables in lines 174,175 are never used, and can
               be safely removed from the =train= function).
   hint 2: =In [1]: zip(range(10), range(10)[1:])= 
               =Out[1]: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)]=
   
9) In line 5, where is $score(act(i))$ taken from?


[easyfirst.py]: sec-9-1

7.2.3 Learning Algorithm and Feature Representation 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    see [Perceptron Classifier]
    

    [Perceptron Classifier]: sec-7-2-5

7.2.4 Computational Complexity and Efficient Implementation 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     
1) From the article (pg. 5): 
   "The parsing algorithm (Algorithm 1) begins with
   $n + 1$ disjoint structures (the words of the sentence +
   ROOT symbol), and terminates with one connected
   structure. Each iteration of the main loop connects
   two structures and removes one of them, and so the
   loop repeats for exactly $n$ times.
   The $argmax$ in line 5 selects the maximal scoring
   action/location pair. At iteration $i$, there are $n - 1$
   locations to choose from, and a naive computation of
   the $argmax$ is $O(n)$, resulting in an $O(n^{2})$ algorithm."
   Is the algorithm really $O(n^{2})$? 
   *Why? What is the non-naive computation?*

2) From the article (pg. 5: footnote 4): 
   "Indeed, in our implementation we do not use a heap, and
   opt instead to find the argmax using a simple O(n) max
   operation. This $O(n^2)$ algorithm is faster in practice than the heap 
   based one, as both are dominated by the $O(n)$ feature extraction,
   while the cost of the $O(n)$ max calculation is negligible 
   compared to the constants involved in heap maintenance." 
   *a) What should have been the complexity using the heap?* 
   *b) Can you think of a situation in which a heap should be used in the
   implementation?*

3) Beam search parsers are only mentioned in Table 1 in the
   article, and the quoted article was published 2 years prior. Do you
   think this might imply at the identity of the identity of the
   editor/reviewers of the NAACL article (this one)? :)

    

7.2.5 TODO Perceptron Classifier 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    

7.2.6 Cython 
~~~~~~~~~~~~~
   1) First, if you are not yet familiar with [Cython], it's highly
      recommended you read the [docs] first, or minimally (and
      reasonably) [the tutorial (pdf)]. A /very/ minimal [Cython Primer] is given
      below.

   2) In the file [ml.pyx] below, what classes are created?

   3) Which of them are python-only? Why?

   4) There are several "data" classes: =DoublesArr=,
      =MulticlassModel=, =MulticlassParamData=, =MultitronParameters=,
      =ParamData=, =PerceptronParameters=. What is each of them used for?

   5) Where in the code is the Perceptron initialized? What are the
      initial values?

   6) Where is the learning (adjustment of parameters) happening?

   7) In line 518, the =score= function is defined. Follow the code,
      and answer these questions:
      a) How is the score calculated?
      b) When will the =except= clause (lines 528-529) be executed?
      c) Can you re-write the function using a functional (rather than
      the current imperative) style?



    
  

      [Cython]: http://cython.org
      [docs]: http://docs.cython.org/
      [the tutorial (pdf)]: http://conference.scipy.org/proceedings/SciPy2009/paper_1/full_text.pdf
      [Cython Primer]: sec-8
      [ml.pyx]: sec-9-2

8 Cython Primer 
----------------
    From [Wikipedia]: 
    "Cython is a programming language to simplify writing C and C++
    extension modules for the CPython Python runtime.[1] Strictly
    speaking, Cython syntax is a superset of Python syntax
    additionally supporting: 
    + Direct calling of C functions, or C++ functions/methods, from Cython code.
    + Strong typing of Cython variables, classes, and class attributes
      as C types. 
    Cython compiles to C or C++ code rather than Python, and the
    result is used as a Python Extension Module or as a stand-alone
    application embedding the CPython runtime."

    The importance of Cython in the implementation of the easy-first
    algorithm, is that it allows a fast and optimized version of
    python to be executed. This is not an accurate statement, but it
    suffices for this paper.

    The Cython code, is used to create the =ml= module, and is found
    in the file =ml.pyx=. This file is *compiled* to the file =ml.c=,
    but for clarity, read the =.pyx= file, not the C one.

    If you have not read any of the links above, here is the bare
    minimum you need to know in order to read =ml.pyx=: 
    1) =cimport= means 'import from a C library.'
    2) =cdef= means 'define in C -- usually it'll be a function, a
       (python) class, or a variable definition, and will have a type.'
    3) Types are declared. To declare (for example) a pointer to a
       double, either =cdef double* double_ptr= or =<double *>
       func(args)= is used.
    4) =cpdef= means 'this is defined in a way that will be accessible
       from both python and c.' In order to define a python-only
       function, the usual =def= applies.
    5) A return type is declared for C functions. For example, 
       =cpdef object get_scores(self,list features):= (taken from =ml.pyx= line 133) means: the
       function =get_scores= is defined in both python and C. it will
       *return an object*, and will receive a =python list= type bound
       to the =features= variable. As in python, =self= means it's a method
    6) =__cinit__= is the class constructor (python's =__init__=) when
       executed from C. Think of it as =__init__=.
    7) =__dealloc__= is the class destructor.
       
    



    [Wikipedia]: https://en.wikipedia.org/wiki/Cython

9 Code 
-------

9.1 easyfirst.py: 
==================
[https://gist.github.com/3467275]


9.2 ml.pyx: 
============
[https://gist.github.com/3566683]


[1] DEFINITION NOT FOUND: 3


