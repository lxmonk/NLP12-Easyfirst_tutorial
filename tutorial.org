#+TITLE:     Tutorial: An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing -- writing the code
#+AUTHOR:    Aviad Reich
#+EMAIL:     avi.rei@gmail.com
#+DATE:      2012-07-21 Sat
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:

#+STYLE: <link rel="stylesheet" type="text/css" href="stylesheets/nlp.css" media="screen, projection" />
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:

* How to contribute to this document
  
This document exists in XHTML, latex, text and org-mode
versions. Editing the first three is straightforward, but will force
all future contributors (if such will exist) to use the same
version/format, since this will break the compatibility of the
different formats. Do this only if you're sure the future contributors
will be OK with it, or you know they will not exist (for example: you
are now completing the document). If you have done so, be sure to
either label the other versions as outdated/irrelevant, or simply
delete them.

Another option is to edit the text file 'tutorial.org' in you favorite
text editor. If you ignore lines 5-15, it's pretty much
straightforward to understand the usage: 
The number of left-aligned '*' before a line determine it's `depth' in
the document (more is deeper), and elements are automatically nested in
the immediate higher level item. 

if you want $\TeX{}$, or $\LaTeX$,
include its code inside adjacent dollar signs: \\
=$[latex code here]$=.
Just look at 'tutorial.org' for examples.

To use the other many wonderful, yet simple to use, features of
org-mode, like auto-numbering of items, footnotes and others, 
it's recommended you read the [[http://orgmode.org/][orgmode website]] and the (relevant) [[http://orgmode.org/org-mode-documentation.html][docs]],
or the tutorial that comes with it as part of Emacs, or [[http://orgmode.org/worg/org-tutorials/][these
tutorials]].

- Exporting :: To export to HTML, tex and text once you're done, open
               Emacs (you  know where to get it), press Ctrl-x Ctrl-f
               and type the path to the file. Then, press Ctrl-c
               Ctrl-e h (for HTML), Ctrl-c Ctrl-e p (for pdf), Ctrl-c
               Ctrl-e l (for latex), Ctrl-c Ctrl-e u (for unicode
               text, a for ascii). Many other export format exist -
               you'll find it in the ``Emacs-style popup''
               window. \\
               *MAKE SURE YOU EXPORT IN ALL FORMATS ONCE
               YOU'RE DONE, SO COMPATIBILITY IS KEPT.*

As this is written (August 2012), I care about this document, and
would be happy to extend my help if it's wanted. To email me use the
first 3 letters of `Aviad', followed by a dot ('.') and the
first 3 of `Reich'. Then mail me at: =[what-you-got]@gmail.com=.

Additionally, there is a github repo:
[[https://github.com/lxmonk/NLP12-Easyfirst_tutorial]], that you can clone
or fork. If you do, and you've created a new one - change this
address. Otherwise, let me know and I'll update.



* An intro to dep parsing

** Formats of the data
** Example sentences
** Rendering of trees in text and graphically

* Intro to dep parsing using transition based parsing

**  How shift reduce works
** Turning a tree into a sequence of shift reduce transitions

* Malt like parsing

** Training a classifier to learn which transition is best at each step
** Typical features used for malt

* Evaluation methods for dep parsing

* Evaluation of our malt parser

* Easy First

** Read the paper

The article: \\
*Easy First Dependency Parsing of Modern Hebrew*, \\
   Yoav Goldberg and Michael Elhadad, \\
   /SPMRL 2010 (NAACL Workshop on Statistical Parsing of
   Morphologically-rich Languages)/


It can be obtained from [[http://www.cs.bgu.ac.il/~yoavg/publications/naacl2010dep.pdf][Yoav Goldberg's BGU webpage]], or at the acm
website: 
http://dl.acm.org/citation.cfm?id=1857999.1858114.


   
** Quiz on the paper

*** Introduction and Easy-First

1) From the article (pg. 1): \\
     "Current dependency parsers can be categorized
     into three families: *local-and-greedy transition-based parsers*
     (e.g., MALTPARSER (Nivre et al., 
     2006)), *globally optimized graph-based parsers*
     (e.g., MST P ARSER (McDonald et al., 2005)), and
     *hybrid systems* (e.g., (Sagae and Lavie, 2006b;
     Nivre and McDonald, 2008)), which combine the
     output of various parsers into a new and improved
     parse, and *which are orthogonal to our approach*."  (no emphasis   
   in the original text) \\
   _Who is orthogonal to the authors' approach? Why?_ \\
   - a :: Local-and-greedy transition-based parsers
   - b :: Globally optimized graph-based parsers
   - c :: Hybrid systems
   - d :: All of the above
   - e :: None of the above
   
2) According to the article, what are the shortcomings of
   transition-based parsers? Which of these have been addressed by the
   easy-first parser? How?

3) One might claim that transition-based parsers suffer from an
   imbalance, in relation to the knowledge they have about the
   sentence as it's being parsed. How might easy-first help to mend
   this? 
   
4) Why are transition-based parsers often restricted to only a limited
   look-ahead window? 
   
5) When will we prefer a globally optimized graph-based parser over a
   transition-based parser? When would a transition-based one be more
   appropriate? 
   
6) The article states that easy-first is a greedy algorithm. What are
   the benefits of this fact? 

7) How are transition-based parsers different than humans when
   dependency-parsing a sentence? 

8) Do humans annotate a sentence in a way similar to graph-based
   parsers? 

9) Of the three parser classes (transition, graph-based and
   easy-first), which is the most similar to a human's way of
   annotating a sentence? 

10) In your opinion, is being similar to the human way of parsing a
    sentence a positive or a negative approach to the problem? Why?

*** Parsing Algorithm

1) Look at figure 1 in the article. In each step, how is the action
    to be performed chosen?

2) In step 1 (figure 1), if the bold number *403* was instead 136,
   what action would have been performed? 

3) What is the range of values for the arc "brown --> fox", assuming
   all others remain unchanged, that will lead to the same parse tree?
   What is the range for the arcs "a --> brown" and "joy --> with"
   that will lead to the same parse tree?

4) Assuming the *difficulty* of choosing an action is measured by the
   difference between the two highest arc's score. On which step was
   making this decision hardest? Can you "feel" this difficulty trying
   to parse the sentence yourself?

5) Algorithm 1.1 is identical to algorithm 1 (see below), but in it, line 3 is *changed* to
   $pending = p_{1} \ldots p_{n-1} \leftarrow w_{1} \ldots w_{n-1}$.
   What will be the first step in parsing the sentence "a brown fox
   jumped with joy" in which the two algorithms will diverge? (hint:
   use figure 1) 
   In general, what will this change cause? \\
   file:images/Algorithm1.png]] \\
   
6) What does the function EdgeFor do? How?
   
7) Write the loop from line 5 in python.

8) Can you find this loop in the file [[easyfirst.py]]? \\
   + hint 1 :: the variables in lines 174,175 are never used, and can
               be safely removed from the =train= function).\\ 
   + hint 2 :: =In [1]: zip(range(10), range(10)[1:])= \\ 
               =Out[1]: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)]=
   
9) In line 5, where is $score(act(i))$ taken from?

*** Learning Algorithm and Feature Representation 
    see [[Perceptron Classifier]]
    
***  Computational Complexity and Efficient Implementation
     
1) From the article (pg. 5): \\
   "The parsing algorithm (Algorithm 1) begins with
   $n + 1$ disjoint structures (the words of the sentence +
   ROOT symbol), and terminates with one connected
   structure. Each iteration of the main loop connects
   two structures and removes one of them, and so the
   loop repeats for exactly $n$ times.
   The $\arg\max$ in line 5 selects the maximal scoring
   action/location pair. At iteration $i$, there are $n - 1$
   locations to choose from, and a naive computation of
   the $\arg\max$ is $O(n)$, resulting in an $O(n^{2})$ algorithm."\\
   Is the algorithm really $O(n^{2})$? \\
   *Why? What is the non-naive computation?*

2) From the article (pg. 5: footnote 4): \\
   "Indeed, in our implementation we do not use a heap, and
   opt instead to find the argmax using a simple O(n) max
   operation. This $O(n^2)$ algorithm is faster in practice than the heap 
   based one, as both are dominated by the $O(n)$ feature extraction,
   while the cost of the $O(n)$ max calculation is negligible 
   compared to the constants involved in heap maintenance." \\
   *a) What should have been the complexity using the heap?* \\
   *b) Can you think of a situation in which a heap should be used in the
   implementation?*

3) Beam search parsers are only mentioned in the Table 1 in the
   article, and the quoted article was published 2 years prior. Do you
   think this might imply at the identity of the identity of the
   editor/reviewers of the NAACL article (this one)? :)

    
*** <<Perceptron Classifier>>
*** <<Cython>>
    First, if you are not yet familiar with [[http://cython.org][Cython]], it's highly
    recommended you read the [[http://docs.cython.org/][docs]] first, or minimally (and
    reasonably) [[http://conference.scipy.org/proceedings/SciPy2009/paper_1/full_text.pdf][the tutorial (pdf)]]. A /very/ minimal [[Cython Primer]] is given
    below. 
    
* <<Cython Primer>>
    From [[https://en.wikipedia.org/wiki/Cython][Wikipedia]]: \\
    "Cython is a programming language to simplify writing C and C++
    extension modules for the CPython Python runtime.[3] Strictly
    speaking, Cython syntax is a superset of Python syntax
    additionally supporting: 
    + Direct calling of C functions, or C++ functions/methods, from Cython code.
    + Strong typing of Cython variables, classes, and class attributes
      as C types. 
    Cython compiles to C or C++ code rather than Python, and the
    result is used as a Python Extension Module or as a stand-alone
    application embedding the CPython runtime."

    The importance of Cython in the implementation of the easy-first
    algorithm, is that it allows a fast and optimized version of
    python to be executed. This is not an accurate statement, but it
    suffices for this paper.

    The Cython code, is used to create the =ml= module, and is found
    in the file =ml.pyx=. This file is *compiled* to the file =ml.c=,
    but for clarity, read the =.pyx= file, not the C one.

    If you have not read any of the links above, here is the bare
    minimum you need to know in order to read =ml.pyx=: 
    1) =cimport= means 'import from a C library.'
    2) =cdef= means 'define in C -- usually it'll be a function, a
       (python) class, or a variable definition, and will have a type.'
    3) Types are declared. To declare (for example) a pointer to a
       double, either =cdef double* double_ptr= or =<double *>
       func(args)= is used.
    4) =cpdef= means 'this is defined in a way that will be accessible
       from both python and c.' In order to define a python-only
       function, the usual =def= applies.
    5) A return type is declared for C functions. For example, \\
       =cpdef object get_scores(self,list features):= (taken from =ml.pyx= line 133) means: the
       function =get_scores= is defined in both python and C. it will
       *return an object*, and will receive a =python list= type bound
       to the =features= variable. As in python, =self= means it's a method
    6) =__cinit__= is the class constructor (python's =__init__=) when
       executed from C. Think of it as =__init__=.
    7) =__dealloc__= is the class destructor.
       
    


* Code
** <<easyfirst.py>>:
[[https://gist.github.com/3467275]]

#+BEGIN_HTML
<script src="https://gist.github.com/3467275.js?file=easyfirst.py"></script>
#+END_HTML

** <<ml.pyx>>:
[[https://gist.github.com/3566683]]

#+BEGIN_HTML
<script src="https://gist.github.com/3566683.js?file=ml.pyx"></script>
#+END_HTML

