#+TITLE:     Tutorial: An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing -- writing the code
#+AUTHOR:    Aviad Reich
#+EMAIL:     avi.rei@gmail.com
#+DATE:      2012-07-21 Sat
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:

#+STYLE: <link rel="stylesheet" type="text/css" href="stylesheets/nlp.css" media="screen, projection" />
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:

* How to contribute to this document
  
This document exists in XHTML, latex, text and org-mode
versions. Editing the first three is straightforward, but will force
all future contributors (if such will exist) to use the same
version/format, since this will break the compatibility of the
different formats. Do this only if you're sure the future contributors
will be OK with it, or you know they will not exist (for example: you
are now completing the document). If you have done so, be sure to
either label the other versions as outdated/irrelevant, or simply
delete them.

Another option is to edit the text file 'tutorial.org' in you favorite
text editor. If you ignore lines 5-15, it's pretty much
straightforward to understand the usage: 
The number of left-aligned '*' before a line determine it's `depth' in
the document (more is deeper), and elements are automatically nested in
the immediate higher level item. 

if you want $\TeX{}$, or $\LaTeX$,
include its code inside adjacent dollar signs: \\
=$[latex code here]$=.
Just look at 'tutorial.org' for examples.

To use the other many wonderful, yet simple to use, features of
org-mode, like auto-numbering of items, footnotes and others, 
it's recommended you read the [[http://orgmode.org/][orgmode website]] and the (relevant) [[http://orgmode.org/org-mode-documentation.html][docs]],
or the tutorial that comes with it as part of Emacs, or [[http://orgmode.org/worg/org-tutorials/][these
tutorials]].

- Exporting :: To export to HTML, tex and text once you're done, open
               Emacs (you  know where to get it), press Ctrl-x Ctrl-f
               and type the path to the file. Then, press Ctrl-c
               Ctrl-e h (for HTML), Ctrl-c Ctrl-e p (for pdf), Ctrl-c
               Ctrl-e l (for latex), Ctrl-c Ctrl-e u (for unicode
               text, a for ascii). Many other export format exist -
               you'll find it in the ``Emacs-style popup''
               window. \\
               *MAKE SURE YOU EXPORT IN ALL FORMATS ONCE
               YOU'RE DONE, SO COMPATIBILITY IS KEPT.*

As this is written (August 2012), I care about this document, and
would be happy to extend my help if it's wanted. To email me use the
first 3 letters of `Aviad', followed by a dot ('.') and the
first 3 of `Reich'. Then mail me at: =[what-you-got]@gmail.com=.

Additionally, there is a github repo:
[[https://github.com/lxmonk/NLP12-Easyfirst_tutorial]], that you can clone
or fork. If you do, and you've created a new one - change this
address. Otherwise, let me know and I'll update.



* An intro to dep parsing

** Formats of the data
** Example sentences
** Rendering of trees in text and graphically

* Intro to dep parsing using transition based parsing

**  How shift reduce works
** Turning a tree into a sequence of shift reduce transitions

* Malt like parsing

** Training a classifier to learn which transition is best at each step
** Typical features used for malt

* Evaluation methods for dep parsing

* Evaluation of our malt parser

* Easy First

** Read the paper

The article: \\
*Easy First Dependency Parsing of Modern Hebrew*, \\
   Yoav Goldberg and Michael Elhadad, \\
   /SPMRL 2010 (NAACL Workshop on Statistical Parsing of
   Morphologically-rich Languages)/


It can be obtained from [[http://www.cs.bgu.ac.il/~yoavg/publications/naacl2010dep.pdf][Yoav Goldberg's BGU webpage]], or at the acm
website: 
http://dl.acm.org/citation.cfm?id=1857999.1858114.


   
** Quiz on the paper

*** Introduction and Easy-First

1) From the article (pg. 1): \\
     "Current dependency parsers can be categorized
     into three families: *local-and-greedy transition-based parsers*
     (e.g., MALTPARSER (Nivre et al., 
     2006)), *globally optimized graph-based parsers*
     (e.g., MST P ARSER (McDonald et al., 2005)), and
     *hybrid systems* (e.g., (Sagae and Lavie, 2006b;
     Nivre and McDonald, 2008)), which combine the
     output of various parsers into a new and improved
     parse, and *which are orthogonal to our approach*."  (no emphasis   
   in the original text) \\
   _Who is orthogonal to the authors' approach? Why?_ \\
   - a :: Local-and-greedy transition-based parsers
   - b :: Globally optimized graph-based parsers
   - c :: Hybrid systems
   - d :: All of the above
   - e :: None of the above
   
2) According to the article, what are the shortcomings of
   transition-based parsers? Which of these have been addressed by the
   easy-first parser? How?

3) One might claim that transition-based parsers suffer from an
   imbalance, in relation to the knowledge they have about the
   sentence as it's being parsed. How might easy-first help to mend
   this? 
   
4) Why are transition-based parsers often restricted to only a limited
   look-ahead window? 
   
5) When will we prefer a globally optimized graph-based parser over a
   transition-based parser? When would a transition-based one be more
   appropriate? 
   
6) The article states that easy-first is a greedy algorithm. What are
   the benefits of this fact? 

7) How are transition-based parsers different than humans when
   dependency-parsing a sentence? 

8) Do humans annotate a sentence in a way similar to graph-based
   parsers? 

9) Of the three parser classes (transition, graph-based and
   easy-first), which is the most similar to a human's way of
   annotating a sentence? 

10) In your opinion, is being similar to the human way of parsing a
    sentence a positive or a negative approach to the problem? Why?

*** Parsing Algorithm

1) Look at figure 1 in the article. In each step, how is the action
    to be performed chosen?

2) In step 1 (figure 1), if the bold number *403* was instead 136,
   what action would have been performed? 

3) What is the range of values for the arc "brown --> fox", assuming
   all others remain unchanged, that will lead to the same parse tree?
   What is the range for the arcs "a --> brown" and "joy --> with"
   that will lead to the same parse tree?

4) Assuming the *difficulty* of choosing an action is measured by the
   difference between the two highest arc's score. On which step was
   making this decision hardest? Can you "feel" this difficulty trying
   to parse the sentence yourself?

5) In algorithm 1: if line 3 would be *changed* to
   $pending = p_{1} \ldots p_{n-1} \leftarrow w_{1} \ldots w_{n-1}$

*** Perceptron Classifier
*** Cython
    
* Cython primer




